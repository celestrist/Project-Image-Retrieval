%% This file was auto-generated by IPython, do NOT edit
%% Conversion from the original notebook file:
%% Toy_Example.ipynb
%%
\documentclass[11pt,english]{article}

%% This is the automatic preamble used by IPython.  Note that it does *not*
%% include a documentclass declaration, that is added at runtime to the overall
%% document.

\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{graphicx}
\usepackage{ucs}
\usepackage[utf8x]{inputenc}

% needed for markdown enumerations to work
\usepackage{enumerate}

% Slightly bigger margins than the latex defaults
\usepackage{geometry}
\geometry{verbose,tmargin=3cm,bmargin=3cm,lmargin=2.5cm,rmargin=2.5cm}

% Define a few colors for use in code, links and cell shading
\usepackage{color}
\definecolor{orange}{cmyk}{0,0.4,0.8,0.2}
\definecolor{darkorange}{rgb}{.71,0.21,0.01}
\definecolor{darkgreen}{rgb}{.12,.54,.11}
\definecolor{myteal}{rgb}{.26, .44, .56}
\definecolor{gray}{gray}{0.45}
\definecolor{lightgray}{gray}{.95}
\definecolor{mediumgray}{gray}{.8}
\definecolor{inputbackground}{rgb}{.95, .95, .85}
\definecolor{outputbackground}{rgb}{.95, .95, .95}
\definecolor{traceback}{rgb}{1, .95, .95}

% Framed environments for code cells (inputs, outputs, errors, ...).  The
% various uses of \unskip (or not) at the end were fine-tuned by hand, so don't
% randomly change them unless you're sure of the effect it will have.
\usepackage{framed}

% remove extraneous vertical space in boxes
\setlength\fboxsep{0pt}

% codecell is the whole input+output set of blocks that a Code cell can
% generate.

% TODO: unfortunately, it seems that using a framed codecell environment breaks
% the ability of the frames inside of it to be broken across pages.  This
% causes at least the problem of having lots of empty space at the bottom of
% pages as new frames are moved to the next page, and if a single frame is too
% long to fit on a page, will completely stop latex from compiling the
% document.  So unless we figure out a solution to this, we'll have to instead
% leave the codecell env. as empty.  I'm keeping the original codecell
% definition here (a thin vertical bar) for reference, in case we find a
% solution to the page break issue.

%% \newenvironment{codecell}{%
%%     \def\FrameCommand{\color{mediumgray} \vrule width 1pt \hspace{5pt}}%
%%    \MakeFramed{\vspace{-0.5em}}}
%%  {\unskip\endMakeFramed}

% For now, make this a no-op...
\newenvironment{codecell}{}

 \newenvironment{codeinput}{%
   \def\FrameCommand{\colorbox{inputbackground}}%
   \MakeFramed{\advance\hsize-\width \FrameRestore}}
 {\unskip\endMakeFramed}

\newenvironment{codeoutput}{%
   \def\FrameCommand{\colorbox{outputbackground}}%
   \vspace{-1.4em}
   \MakeFramed{\advance\hsize-\width \FrameRestore}}
 {\unskip\medskip\endMakeFramed}

\newenvironment{traceback}{%
   \def\FrameCommand{\colorbox{traceback}}%
   \MakeFramed{\advance\hsize-\width \FrameRestore}}
 {\endMakeFramed}

% Use and configure listings package for nicely formatted code
\usepackage{listingsutf8}
\lstset{
  language=python,
  inputencoding=utf8x,
  extendedchars=\true,
  aboveskip=\smallskipamount,
  belowskip=\smallskipamount,
  xleftmargin=2mm,
  breaklines=true,
  basicstyle=\small \ttfamily,
  showstringspaces=false,
  keywordstyle=\color{blue}\bfseries,
  commentstyle=\color{myteal},
  stringstyle=\color{darkgreen},
  identifierstyle=\color{darkorange},
  columns=fullflexible,  % tighter character kerning, like verb
}

% The hyperref package gives us a pdf with properly built
% internal navigation ('pdf bookmarks' for the table of contents,
% internal cross-reference links, web links for URLs, etc.)
\usepackage{hyperref}
\hypersetup{
  breaklinks=true,  % so long urls are correctly broken across lines
  colorlinks=true,
  urlcolor=blue,
  linkcolor=darkorange,
  citecolor=darkgreen,
  }

% hardcode size of all verbatim environments to be a bit smaller
\makeatletter 
\g@addto@macro\@verbatim\small\topsep=0.5em\partopsep=0pt
\makeatother 

% Prevent overflowing lines due to urls and other hard-to-break entities.
\sloppy

\begin{document}

\section{Toy Example of Image Retrieval(1)}
\subsection{Bag of Visual Words Representation}
I am going to demonstrate how to represent an image using histogram of
its constituent visual words. But before that, I would like to do some
review in order to fix the terminology that I are going to use through
out this article.

To the computer, an image is just an array of pixels( an interesting
article about the common misunderstanding that a pixel is a little
square, written by Alvy Ray Smith, can be found here. ) But this low
level representation corresponds poorly to our perceived `meaning' of
the images - there is a \emph{semantic gap} between our perception and
the representation. Instead of talking about the pixel of an image, we
would like to talk about the colors, textures, shapes, and other visual
features. Along this line of effort, the computer vision(CV) community
has created many ways to describe and quantify different visual
features, which we call \emph{visual descriptors}.

I am going to use the \textbf{Scale-Invariant Feature Transform (SIFT)}
descriptor developed by David Rowe. For our purpose here, we only have
to know that the SIFT algorithm detects various interesting points in an
image, and encodes the local visual feature around the points as
vectors. At this stage of work, we have transformed the raw pixel
represention of an image into a collection of vectors that describe the
local visual features of some interesting points in the image.

Given all the descriptors of a collection of images, those that describe
similar visual features can be grouped together. The resultant groups
can be thought as typical local features. In practice, we will calculate
some kind of `average value' of all the descriptors within a group and
use it to represent the group. These groups or `average values' are
called \textbf{visual words} because they constitue an image similar to
the words constitue a text. At the previous stage, we have transform an
image into a collection of descriptors; at this stage, we have assigned
each descriptors a group. As a result, we can count how many times a
typical visual feature, represented by the average value of the group,
occurs in any given image. In another word, we have constructed a
histogram of typical visual features for every images. In the machine
learning literature, the grouping of the descriptors is called
\textbf{clustering}; the assignment of descriptors to their
corresponding group, \textbf{vector quantization}; and the histogram
that represents the image, \textbf{bag of visual words representation}
or \textbf{vector space representation}.

I have included the Python code that implements the ideas above here.
You can execute them inside IPython notebook to verify the result.

\begin{codecell}
\begin{codeinput}
\begin{lstlisting}
#loading required libraries

import os
import sift
from sklearn.preprocessing import normalize
from sklearn import cluster
import cPickle
import scipy.sparse as sp
\end{lstlisting}
\end{codeinput}
\end{codecell}
I have selected 10 pictures from each of the 10 first categories of
Caltech101 datasets. They are inside the folder images/Training\_Images.
Here are 3 examples from the training dataset.

 \emph{(you might not be able to see the images if you are using the pdf version of this file)}

Next, I calculate the SIFT descriptor for each of the images and store
them in the same folder

\begin{codecell}
\begin{codeinput}
\begin{lstlisting}
# the conversion takes some time, so you can skip this step
# i have included the .sift file in this repository
impath = '../images/Training_Images/'
feapath = '../images/sift_file/'

imlist = os.listdir(impath)[1:]
sift_list = map(lambda x: x.replace('jpg','sift'),imlist)

for i in range(len(imlist)):
    sift.process_image(os.path.join(impath,imlist[i]),os.path.join(feapath,sift_list[i]))
\end{lstlisting}
\end{codeinput}
\end{codecell}
To prepare for clustering, I stack the descriptor together and normalize
it.

\begin{codecell}
\begin{codeinput}
\begin{lstlisting}
# v is the list of all sift descriptor
v = [sift.read_features_from_file(os.path.join(feapath,sift_list[i]))[1] for i in arange(100)]

# we stack all the descriptors in the list v together
# the result is a matrix
feature_vector = vstack(v)

# we normalize the matrix
n_feature_vector = normalize(feature_vector)

#save the matrix for further use
save('n_feature_vector',n_feature_vector)
\end{lstlisting}
\end{codeinput}
\end{codecell}
Now we perform the clustering using k-mean clustering algorithm
implemented in the scikit library.

\begin{codecell}
\begin{codeinput}
\begin{lstlisting}
k = cluster.KMeans(k=100)
k.fit(n_feature_vector[::10])
\end{lstlisting}
\end{codeinput}
\begin{codeoutput}
\begin{verbatim}
KMeans(copy_x=True, init='k-means++', k=100, max_iter=300, n_init=10,
    n_jobs=1, precompute_distances=True,
    random_state=<mtrand.RandomState object at 0x24e830>, tol=0.0001,
    verbose=0)
\end{verbatim}
\end{codeoutput}
\end{codecell}
Save the result to disk since it is computationaly intensive to train
the learner.

\begin{codecell}
\begin{codeinput}
\begin{lstlisting}
with open('learner.pkl','wb') as f:
    cPickle.dump(k,f)
\end{lstlisting}
\end{codeinput}
\end{codecell}
Now we can represent the images using the 100 visual words.

\begin{codecell}
\begin{codeinput}
\begin{lstlisting}
with open('learner.pkl','rb') as f:
    k = cPickle.load(f)
\end{lstlisting}
\end{codeinput}
\end{codecell}
\begin{codecell}
\begin{codeinput}
\begin{lstlisting}
# we normalize the sift descriptors so that 
# they are comparable to our learning result.
n_n = map(normalize,v)

# we assign the corresponding visual word to
# each of the descriptors
q = [k.predict(x) for x in n_n]
\end{lstlisting}
\end{codeinput}
\end{codecell}
Anticipating the result is sparse array, I write some helper function to
calculate the norm and cosine distance.

\begin{codecell}
\begin{codeinput}
\begin{lstlisting}
# count the number of typical features,
# and form the bag of visual words representation
def vector_sparray(vec):
    array = sp.dok_matrix((100,1),dtype=float32)
    for v in vec:
        if (v,0) in array:
            array[v,0] = array[v,0] + 1
        else:
            array[v,0] = 1
            
    return array  

def snorm(svector):
    return sqrt(svector.T.dot(svector)[0,0])

def cosine_distance(v1,v2):
    return 1 - (v1.T.dot(v2))[0,0]/(snorm(v1)*snorm(v2))
\end{lstlisting}
\end{codeinput}
\end{codecell}
\begin{codecell}
\begin{codeinput}
\begin{lstlisting}
# the bag of visual words representation for all images
Histogram = map(vector_sparray,q)

# the representation for the first image
h1 = Histogram[0]
\end{lstlisting}
\end{codeinput}
\end{codecell}
\begin{codecell}
\begin{codeinput}
\begin{lstlisting}
#calculate the cosine distance from h1 to every other images
distance = map(lambda x: cosine_distance(h1,x),Histogram)
\end{lstlisting}
\end{codeinput}
\end{codecell}
\begin{codecell}
\begin{codeinput}
\begin{lstlisting}
# return the top 10 results that are nearest to the first image
d=dict(enumerate(distance))
sorted(d,key=d.get)[:10]
\end{lstlisting}
\end{codeinput}
\begin{codeoutput}
\begin{verbatim}
[0, 8, 1, 3, 6, 2, 5, 91, 55, 90]
\end{verbatim}
\end{codeoutput}
\end{codecell}
The first 10 images in the dataset is similar, and our top 10 result
contains 7 of them.

\subsubsection{TODO : More data analysis to justify the representation
}
\end{document}
