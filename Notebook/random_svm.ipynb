{
 "metadata": {
  "name": "random_svm"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import sklearn.random_projection as rp\n",
      "import numpy as np\n",
      "import random\n",
      "from sklearn.datasets import make_classification\n",
      "from sklearn.svm import SVC\n",
      "from sklearn import cross_validation\n",
      "from sklearn.preprocessing import normalize\n",
      "from sklearn import gaussian_process\n",
      "from sklearn.naive_bayes import GaussianNB\n",
      "from sklearn import tree\n",
      "from sklearn.ensemble import RandomForestClassifier, ExtraTreesClassifier, GradientBoostingClassifier\n",
      "from collections import Counter\n",
      "\n",
      "def make_projector(random_seed,dimension):\n",
      "    return rp.GaussianRandomProjection(random_state=random_seed,n_components=dimension)\n",
      "\n",
      "def ensemble_trainer(X,y,classifiers,transformers):\n",
      "    Reduced_Xs= map(lambda t: t.fit_transform(X),transformers) \n",
      "    return map(lambda c,x: c.fit(x,y),classifiers,Reduced_Xs)\n",
      "\n",
      "def predict_individually(X,ensemble,transformers):\n",
      "    Reduced_Xs= map(lambda t: t.fit_transform(X),transformers)\n",
      "    return zip(*map(lambda c,x: c.predict(x), ensemble, Reduced_Xs))\n",
      "    \n",
      "def predict(X,ensemble,transformers,calculate_prob=False):\n",
      "    \n",
      "    Reduced_Xs= map(lambda t: t.fit_transform(X),transformers)\n",
      "    result = map(Counter,zip(*map(lambda c,x: c.predict(x), ensemble, Reduced_Xs)))\n",
      "    if not calculate_prob:\n",
      "        return map(lambda x:x.most_common(1)[0][0],result)\n",
      "    else:\n",
      "        \n",
      "        return map(lambda x: map(lambda y: {y:x[y]/float(N_ENSEMBLES)},x) ,result)\n",
      "        \n",
      "def score(classifier, transformer, X, true_y):\n",
      "    y = predict(X,classifier,transformer)\n",
      "    return np.mean(y == true_y)\n",
      "\n",
      "class identity_transformer():\n",
      "    def __init__(self):\n",
      "        pass\n",
      "    def fit_transform(self,x):\n",
      "        return x\n",
      "    \n",
      "    \n",
      "X,y = make_classification(n_samples=100,n_features=20,n_informative=10)\n",
      "kfold = cross_validation.KFold(100,10)\n",
      "\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 1
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "N_ENSEMBLES = 50\n",
      "RANDOM_REDUCED_DIMENSION = np.random.randint(low=1,high=20,size=N_ENSEMBLES)\n",
      "RANDOM_STATES = np.random.randint(low=1,high=200,size=N_ENSEMBLES)\n",
      "RANDOM_PENALTY = np.random.random(N_ENSEMBLES)*10\n",
      "Classifiers = map(lambda x,p : SVC(kernel=\"linear\",C=p), range(N_ENSEMBLES),RANDOM_PENALTY)\n",
      "g_Classifiers = map(lambda x:gaussian_process.GaussianProcess(),range(N_ENSEMBLES))\n",
      "b_Classifiers = map(lambda x:GaussianNB(),range(N_ENSEMBLES))\n",
      "t_Classifiers = map(lambda x:tree.DecisionTreeClassifier(),range(N_ENSEMBLES))\n",
      "r_Classifiers = map(lambda x:RandomForestClassifier(n_estimators=10),range(N_ENSEMBLES))\n",
      "e_classifiers = map(lambda x:ExtraTreesClassifier(),range(N_ENSEMBLES))\n",
      "m_classifiers = random.sample((g_Classifiers + b_Classifiers + t_Classifiers + r_Classifiers),N_ENSEMBLES)\n",
      "Transformers = map(lambda s, d: make_projector(s,d),RANDOM_STATES,RANDOM_REDUCED_DIMENSION)\n",
      "\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 2
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "cls = GradientBoostingClassifier()\n",
      "np.mean([cls.fit(X[train],y[train]).score(X[test],y[test]) for train,test in kfold])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "pyout",
       "prompt_number": 3,
       "text": [
        "0.75000000000000011"
       ]
      }
     ],
     "prompt_number": 3
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "cls = SVC(kernel=\"linear\")\n",
      "np.mean([cls.fit(X[train],y[train]).score(X[test],y[test]) for train,test in kfold])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "pyout",
       "prompt_number": 4,
       "text": [
        "0.87000000000000011"
       ]
      }
     ],
     "prompt_number": 4
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "np.mean([score(ensemble_trainer(X[train],y[train],Classifiers,Transformers),Transformers,X[test],y[test]) \n",
      "     for train,test in kfold])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "pyout",
       "prompt_number": 5,
       "text": [
        "0.8500000000000002"
       ]
      }
     ],
     "prompt_number": 5
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "np.mean([score(ensemble_trainer(X[train],y[train],b_Classifiers,Transformers),Transformers,X[test],y[test]) \n",
      "     for train,test in kfold])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "pyout",
       "prompt_number": 6,
       "text": [
        "0.81000000000000016"
       ]
      }
     ],
     "prompt_number": 6
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "np.mean([score(ensemble_trainer(X[train],y[train],t_Classifiers,Transformers),Transformers,X[test],y[test]) \n",
      "     for train,test in kfold])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "pyout",
       "prompt_number": 7,
       "text": [
        "0.84000000000000008"
       ]
      }
     ],
     "prompt_number": 7
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "np.mean([score(ensemble_trainer(X[train],y[train],r_Classifiers,Transformers),Transformers,X[test],y[test]) \n",
      "     for train,test in kfold])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "pyout",
       "prompt_number": 8,
       "text": [
        "0.84000000000000008"
       ]
      }
     ],
     "prompt_number": 8
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "np.mean([score(ensemble_trainer(X[train],y[train],e_classifiers,Transformers),Transformers,X[test],y[test]) \n",
      "     for train,test in kfold])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "pyout",
       "prompt_number": 9,
       "text": [
        "0.84000000000000008"
       ]
      }
     ],
     "prompt_number": 9
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "np.mean([score(ensemble_trainer(X[train],y[train],m_classifiers,Transformers),Transformers,X[test],y[test]) \n",
      "     for train,test in kfold])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "pyout",
       "prompt_number": 10,
       "text": [
        "0.81999999999999995"
       ]
      }
     ],
     "prompt_number": 10
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}