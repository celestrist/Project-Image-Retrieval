{
 "metadata": {
  "name": "Toy_Example"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "Toy Example of Image Retrieval(1)"
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Bag of Visual Words Representation"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "I am going to demonstrate how to represent an image using histogram of its constituent visual words. But before that, I would like to do some review in order to fix the terminology that I are going to use through out this article.\n",
      "\n",
      "To the computer, an image is just an array of pixels( an interesting article about the common misunderstanding that a pixel is a little square, written by Alvy Ray Smith, can be found <a href='http://alvyray.com/Memos/CG/Microsoft/6_pixel.pdf'>here.</a> ) But this low level representation corresponds poorly to our perceived 'meaning' of the images - there is a *semantic gap* between our perception and the representation. Instead of talking about the pixel of an image, we would like to talk about the colors, textures, shapes, and other visual features. Along this line of effort, the computer vision(CV) community has created many ways to describe and quantify different visual features, which we call *visual descriptors*.\n",
      "\n",
      "I am going to use the **Scale-Invariant Feature Transform (SIFT)** descriptor developed by David Rowe. For our purpose here, we only have to know that the SIFT algorithm detects various interesting points in an image, and encodes the local visual feature around the points as vectors. At this stage of work, we have transformed the raw pixel represention of an image into a collection of vectors that describe the local visual features of some interesting points in the image.\n",
      "\n",
      "Given all the descriptors of a collection of images, those that describe similar visual features can be grouped together. The resultant groups can be thought as typical local features. In practice, we will calculate some kind of 'average value' of all the descriptors within a group and use it to represent the group. These groups or 'average values' are called **visual words** because they constitue an image similar to the words constitue a text. At the previous stage, we have transform an image into a collection of descriptors; at this stage, we have assigned each descriptors a group. As a result, we can count how many times a typical visual feature, represented by the average value of the group, occurs in any given image. In another word, we have constructed a histogram of typical visual features for every images. In the machine learning literature, the grouping of the descriptors is called **clustering**; the assignment of descriptors to their corresponding group, **vector quantization**; and the histogram that represents the image, **bag of visual words representation** or **vector space representation**.\n",
      "\n",
      "I have included the Python code that implements the ideas above here. You can execute them inside IPython notebook to verify the result."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#loading required libraries\n",
      "\n",
      "import os\n",
      "import sift\n",
      "from sklearn.preprocessing import normalize\n",
      "from sklearn import cluster\n",
      "import cPickle\n",
      "import scipy.sparse as sp"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 1
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "I have selected 10 pictures from each of the 10 first categories of Caltech101 datasets. They are inside the folder images/Training_Images. Here are 3 examples from the training dataset."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "<div><img src='files/images/airplane_image_0001.jpg'/>  <img src='files/images/ant_image_0001.jpg'/> <img src='files/images/beaver_image_0004.jpg'/>"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Next, I calculate the SIFT descriptor for each of the images and store them in the same folder"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# the conversion takes some time, so you can skip this step\n",
      "# i have included the .sift file in this repository\n",
      "impath = '../images/Training_Images/'\n",
      "feapath = '../images/sift_file/'\n",
      "\n",
      "imlist = os.listdir(impath)[1:]\n",
      "sift_list = map(lambda x: x.replace('jpg','sift'),imlist)\n",
      "\n",
      "for i in range(len(imlist)):\n",
      "    sift.process_image(os.path.join(impath,imlist[i]),os.path.join(feapath,sift_list[i]))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 2
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "To prepare for clustering, I stack the descriptor together and normalize it."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# v is the list of all sift descriptor\n",
      "v = [sift.read_features_from_file(os.path.join(feapath,sift_list[i]))[1] for i in arange(100)]\n",
      "\n",
      "# we stack all the descriptors in the list v together\n",
      "# the result is a matrix\n",
      "feature_vector = vstack(v)\n",
      "\n",
      "# we normalize the matrix\n",
      "n_feature_vector = normalize(feature_vector)\n",
      "\n",
      "#save the matrix for further use\n",
      "save('n_feature_vector',n_feature_vector)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 3
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Now we perform the clustering using k-mean clustering algorithm implemented in the scikit library."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "k = cluster.KMeans(k=100)\n",
      "k.fit(n_feature_vector[::10])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "pyout",
       "prompt_number": 4,
       "text": [
        "KMeans(copy_x=True, init='k-means++', k=100, max_iter=300, n_init=10,\n",
        "    n_jobs=1, precompute_distances=True,\n",
        "    random_state=<mtrand.RandomState object at 0x24e830>, tol=0.0001,\n",
        "    verbose=0)"
       ]
      }
     ],
     "prompt_number": 4
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Save the result to disk since it is computationaly intensive to train the learner."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "with open('learner.pkl','wb') as f:\n",
      "    cPickle.dump(k,f)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 25
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Now we can represent the images using the 100 visual words."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "with open('learner.pkl','rb') as f:\n",
      "    k = cPickle.load(f)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 2
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# we normalize the sift descriptors so that \n",
      "# they are comparable to our learning result.\n",
      "n_n = map(normalize,v)\n",
      "\n",
      "# we assign the corresponding visual word to\n",
      "# each of the descriptors\n",
      "q = [k.predict(x) for x in n_n]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 5
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Anticipating the result is sparse array, I write some helper function to calculate the norm and cosine distance."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# count the number of typical features,\n",
      "# and form the bag of visual words representation\n",
      "def vector_sparray(vec):\n",
      "    array = sp.dok_matrix((100,1),dtype=float32)\n",
      "    for v in vec:\n",
      "        if (v,0) in array:\n",
      "            array[v,0] = array[v,0] + 1\n",
      "        else:\n",
      "            array[v,0] = 1\n",
      "            \n",
      "    return array  \n",
      "\n",
      "def snorm(svector):\n",
      "    return sqrt(svector.T.dot(svector)[0,0])\n",
      "\n",
      "def cosine_distance(v1,v2):\n",
      "    return 1 - (v1.T.dot(v2))[0,0]/(snorm(v1)*snorm(v2))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 19
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# the bag of visual words representation for all images\n",
      "Histogram = map(vector_sparray,q)\n",
      "\n",
      "# the representation for the first image\n",
      "h1 = Histogram[0]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#calculate the cosine distance from h1 to every other images\n",
      "distance = map(lambda x: cosine_distance(h1,x),Histogram)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 21
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# return the top 10 results that are nearest to the first image\n",
      "d=dict(enumerate(distance))\n",
      "sorted(d,key=d.get)[:10]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "pyout",
       "prompt_number": 27,
       "text": [
        "[0, 8, 1, 3, 6, 2, 5, 91, 55, 90]"
       ]
      }
     ],
     "prompt_number": 27
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The first 10 images in the dataset is similar, and our top 10 result contains 7 of them."
     ]
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "TODO : More data analysis to justify the representation\n"
     ]
    }
   ],
   "metadata": {}
  }
 ]
}